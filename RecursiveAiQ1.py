#Understanding Stochastic Gradient Descent. In this exercise, we want to implement Stochastic Gradient descent without 
#using a built-in optimizer, to understand how parameters are optimized at the low level. Let's consider a toy 
#problem of Logistic Regression. The data generation process is such that we have one floating point variable  x∈[0,1] ,
# and one categorical variable  y∈{0,1} . The Logistic Regression problem is to find the optimal parameters for 
#the logistic function to predict the probability of  y  given  x .

#In the image above, the x and y axis correspond to the variables defined previously, with the red "dots" the true 
#data as generated by the process defined below, and the blue line is the optimal logistic decision function learned 
#from the data.

#The optimal parameters can be found numerically using gradient descent. In this case, we will be using SGD 
# (batched gradient descent) to optimize the two parameters in the logistic function,  β1,β2 .
#If  p(x)=11+e−(β1+β2x) , then we want to find  β1,β2  such that the negative log likelihood is minimized. 
# In this case,  ℓ=∑Kk=1(ykln(pk)+(1−yk)ln(1−pk)) , and the SGD formula is simply:  βi=βi−1−α∗∇ℓ .
# The data generation process as well as the pseudo-code is provided for you below, your task is to fill in the 
# appropriate pytorch code. Do not use the built-in optimizer, implement SGD using tensor operations alone.


# # # """"

import numpy as np
import torch
import matplotlib.pyplot as plt

# Generate synthetic data
def generate_data():
    data = torch.rand(1000, 2)  # Random data points (x1, x2)
    label = ((data[:, 0] + 0.3 * data[:, 1]) > 0.5).to(torch.float32)  # Labels based on a threshold
    return data[:, 0], label

# Generate input and labels
input, label = generate_data()

# Make minibatches (batch size = 32)
inputs = torch.split(input, 32)
labels = torch.split(label, 32)

# Define parameters to optimize
b1 = torch.autograd.Variable(torch.tensor([0.01]), requires_grad=True)
b2 = torch.autograd.Variable(torch.tensor([0.01]), requires_grad=True)

learning_rate = 0.1  # Step size for gradient descent

# Training loop
for epoch in range(15):
    total_loss = 0  # To track the total loss for this epoch
    for x, y in zip(inputs, labels):
        # Calculate p(x) = 1 / (1 + exp(-(b1 + b2 * x)))
        logits = b1 + b2 * x
        p_x = torch.sigmoid(logits)
        
        # Calculate the negative log-likelihood loss
        loss = -(y * torch.log(p_x) + (1 - y) * torch.log(1 - p_x)).mean()
        total_loss += loss.item()  # Add to total loss for this epoch

        # Calculate gradients of the loss w.r.t. b1 and b2
        loss.backward()

        # Update parameters b1 and b2 using SGD
        with torch.no_grad():
            b1 -= learning_rate * b1.grad
            b2 -= learning_rate * b2.grad

        # Zero gradients after the update
        b1.grad.zero_()
        b2.grad.zero_()
    
    # Print out the loss value for this epoch
    print(f"Epoch {epoch+1}: Loss = {total_loss:.4f}, b1 = {b1.item():.4f}, b2 = {b2.item():.4f}")

# Reproduce the image to validate results
with torch.no_grad():
    plt.scatter(input.numpy(), label.numpy(), color='red', label='Data')  # True data
    x_plot = torch.linspace(0, 1, 100)  # Generate x values for the curve
    y_plot = torch.sigmoid(b1 + b2 * x_plot)  # Predicted probabilities
    plt.plot(x_plot.numpy(), y_plot.numpy(), label='Model', color='blue')  # Logistic regression curve
    plt.legend()
    plt.show()
